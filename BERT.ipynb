{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-q3ILXXiVbpTRz876UocxReAdlSBJQlP",
      "authorship_tag": "ABX9TyMM0F1pLrhRHlhwx685cN/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naokityokoyama/HDC/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode num2words evaluate"
      ],
      "metadata": {
        "id": "QHp9Wp9uIvMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import zipfile\n",
        "from unidecode import unidecode\n",
        "import string\n",
        "from num2words import num2words\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import DatasetDict, Dataset, load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "cgBjOqVtIWsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_origin = '/content/drive/MyDrive/uff/fake.zip'\n",
        "path_destino = '/content/'\n",
        "with zipfile.ZipFile(path_origin, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(path_destino)"
      ],
      "metadata": {
        "id": "vtK5oaaYImeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build dataset\n",
        "\n",
        "df_fake = pd.read_csv('/content/fakes.csv')[['text']]\n",
        "df_fake['target'] = 1\n",
        "df_true = pd.read_csv('/content/true.csv')[['text']]\n",
        "df_true['target'] = 0\n",
        "df = pd.concat([df_fake, df_true]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "n0netAYBI64y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o tamanho da amostra\n",
        "sample_size = 10000\n",
        "\n",
        "# Criar uma amostra balanceada\n",
        "df = df.groupby(\"target\", group_keys=False).apply(lambda x: resample(x, n_samples=sample_size // df[\"target\"].nunique(), random_state=42))\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Qa70nMpJI9tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean\n",
        "\n",
        "def n2w(texto:str)->str:\n",
        "  padrao = r\"\\d+\"\n",
        "  numeros = re.findall(padrao, texto)\n",
        "  for numero in numeros:\n",
        "    extenso = num2words(numero, lang='pt')\n",
        "    texto = texto.replace(numero, extenso)\n",
        "  return texto"
      ],
      "metadata": {
        "id": "WlNnMv6OJFlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for repet in tqdm(range(2)):  #bug para rodar 2x\n",
        "  df['text'] = df['text'].str.lower()\n",
        "  df['text'] = df['text'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "  df['text'] = df['text'].apply(lambda x: ' '.join(x.split()))\n",
        "  df['text'] = df['text'].str.replace('\"', '').str.replace('\\\\', '')\n",
        "  df['text'] = df['text'].apply(n2w)\n",
        "  df['text'] = df['text'].apply(unidecode)"
      ],
      "metadata": {
        "id": "PPIaM5OSJJDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create X and y\n",
        "X = df['text']\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "Q_x4s1f2JNin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "1GxkU6gSK_JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = Dataset.from_dict({'text': X_train, 'labels': y_train})\n",
        "val = Dataset.from_dict({'text': X_val, 'labels': y_val})\n",
        "test = Dataset.from_dict({'text': X_test, 'labels': y_test})\n",
        "dataset_dict = DatasetDict({'train': train, 'test': test})\n",
        "dataset_dict = DatasetDict({'train': train, 'validation': val, 'test': test})\n",
        "num_labels = len(set(np.array(y)))"
      ],
      "metadata": {
        "id": "NZgbn60OLY8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define pre-trained model path\n",
        "model_path = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "# load model tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "S_T-ceqvLc3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
        "                                                           num_labels=num_labels)"
      ],
      "metadata": {
        "id": "DfF7bjM4LnvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all base model parameters\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# unfreeze base model pooling layers\n",
        "for name, param in model.base_model.named_parameters():\n",
        "    if \"pooler\" in name:\n",
        "        param.requires_grad = True"
      ],
      "metadata": {
        "id": "jj8o7cDvL1P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define text preprocessing\n",
        "def preprocess_function(examples):\n",
        "    # return tokenized text with truncation\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# preprocess all datasets\n",
        "tokenized_data = dataset_dict.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "Y5d9dtKlNpFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "pyZe_5iGOzRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "auc_score = evaluate.load(\"roc_auc\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    # get predictions\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # apply softmax to get probabilities\n",
        "    probabilities = np.exp(predictions) / np.exp(predictions).sum(-1,\n",
        "                                                                 keepdims=True)\n",
        "    # use probabilities of the positive class for ROC AUC\n",
        "    positive_class_probs = probabilities[:, 1]\n",
        "    # compute auc\n",
        "    auc = np.round(auc_score.compute(prediction_scores=positive_class_probs,\n",
        "                                     references=labels)['roc_auc'],3)\n",
        "\n",
        "    # predict most probable class\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    # compute accuracy\n",
        "    acc = np.round(accuracy.compute(predictions=predicted_classes,\n",
        "                                     references=labels)['accuracy'],3)\n",
        "\n",
        "    return {\"Accuracy\": acc, \"AUC\": auc}"
      ],
      "metadata": {
        "id": "e-x1mt5LO1pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 32\n",
        "num_epochs = 2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert-phishing-classifier_teacher\",\n",
        "    report_to=\"none\", #disable wandb\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "BRphcnRVO4nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "iMTwG4uWO5Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply model to validation dataset\n",
        "predictions = trainer.predict(tokenized_data[\"validation\"])\n",
        "\n",
        "# Extract the logits and labels from the predictions object\n",
        "logits = predictions.predictions\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Use your compute_metrics function\n",
        "metrics = compute_metrics((logits, labels))\n",
        "print(metrics)\n",
        "\n"
      ],
      "metadata": {
        "id": "t_6y4P4nRCuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}