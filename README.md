O Que √© HDC (Hyperdimensional Computing)?

Hyperdimensional Computing (HDC), ou Computa√ß√£o Hiperdimensional, √© um paradigma inovador inspirado em como o c√©rebro humano processa informa√ß√µes. Diferente da computa√ß√£o tradicional, que depende de opera√ß√µes aritm√©ticas sobre n√∫meros de precis√£o finita, o HDC utiliza vetores de alta dimens√£o, tamb√©m chamados de hipervetores, para representar e manipular dados e conhecimento.

Como Funciona a Computa√ß√£o Hiperdimensional

No HDC, informa√ß√µes s√£o codificadas em vetores com milhares de dimens√µes ‚Äî frequentemente de 10.000 at√© 100.000 dimens√µes ‚Äî com elementos bin√°rios, inteiros ou reais. Esses hipervetores podem ser combinados atrav√©s de opera√ß√µes matem√°ticas simples, como soma, multiplica√ß√£o elemento-a-elemento (binding) e permuta√ß√£o, permitindo representar conceitos complexos de maneira compacta e robusta.

O funcionamento b√°sico do HDC passa por tr√™s etapas principais:

Codifica√ß√£o:
Itens (como palavras, sensores ou categorias) s√£o convertidos em hipervetores "aleat√≥rios", mas reprodut√≠veis.
Composi√ß√£o:
A rela√ß√£o entre itens (ex: palavras numa senten√ßa) √© modelada por opera√ß√µes entre hipervetores, gerando um novo hipervetor que representa essa rela√ß√£o.
Classifica√ß√£o ou Decodifica√ß√£o:
O hipervetor composto pode ser comparado a hipervetores de refer√™ncia usando medidas de similaridade (como cosseno ou Hamming) para inferir informa√ß√µes ou reconhecer padr√µes.
Diferen√ßas em Rela√ß√£o √† Computa√ß√£o Convencional e Redes Neurais

Robustez:
O HDC √© extremamente tolerante a ru√≠dos, falhas e varia√ß√µes nos dados ‚Äî caracter√≠stica alcan√ßada pelo alto n√∫mero de dimens√µes e pela redund√¢ncia da informa√ß√£o.
Simplicidade nas Opera√ß√µes:
As opera√ß√µes entre hipervetores s√£o matematicamente simples e de f√°cil paraleliza√ß√£o, tornando o HDC eficiente para hardware especializado e aplica√ß√µes embarcadas.
Treinamento R√°pido:
Algoritmos de HDC frequentemente requerem uma √∫nica passagem sobre o dado (one-shot learning), ao contr√°rio de redes neurais profundas que demandam treinamento longo e muitos dados.
Interpretabilidade:
O HDC pode oferecer maior transpar√™ncia do que modelos ‚Äúcaixa preta‚Äù, pois cada opera√ß√£o tem significado matem√°tico claro e a representa√ß√£o √© distribu√≠da.
Aplica√ß√µes

O HDC j√° foi aplicado com sucesso em diversas √°reas, como:

Reconhecimento de padr√µes e classifica√ß√£o (NLP, sinais biom√©tricos)
Processamento de linguagem natural (codifica√ß√£o de texto, an√°lise de sentimentos)
Sistemas embarcados para IoT e sensores, onde recursos computacionais s√£o limitados
Conclus√£o

A Computa√ß√£o Hiperdimensional prop√µe uma forma alternativa de manipular dados inspirada em mecanismos cerebrais. Sua alta efici√™ncia, escalabilidade e robustez a tornam uma solu√ß√£o promissora para tarefas onde os modelos tradicionais, como redes neurais profundas, podem ser excessivos ou dif√≠ceis de aplicar. Entre suas maiores vantagens est√£o a facilidade de implementa√ß√£o, baixo custo computacional e maior interpretabilidade ‚Äî caracter√≠sticas-chave para aplica√ß√µes de intelig√™ncia artificial no mundo real.

### PROJETO  

Fake News Detection: Hyperdimensional Computing (HDC) vs BERT
üìí üì∞ ü§ñ

Este reposit√≥rio apresenta uma compara√ß√£o entre algoritmos de Hyperdimensional Computing (HDC) e o modelo BERT aplicado ao problema de detec√ß√£o de fake news em um dataset rotulado (0 = n√£o fake, 1 = fake news). O objetivo √© avaliar o desempenho, interpretabilidade e complexidade de cada abordagem em cen√°rios pr√°ticos.

Notebooks do Projeto

‚Ä¢ Notebook 1: HDC com ngram encoder (BinHD & NeuralHD)  

Implementa o encoder de n-gramas para representar textos em espa√ßos hiperdimensionais.
Avalia dois algoritmos: BinHD (baseado em opera√ß√µes bin√°rias) e NeuralHD (variante neuroinspirada).
Permite observar as diferen√ßas de desempenho e complexidade dos algoritmos HDC.    

‚Ä¢ Notebook 2: HDC com ngram encoder + record encoder (BinHD)  

Adiciona o uso do record encoder para enriquecer as representa√ß√µes hiperdimensionais.
Utiliza o algoritmo BinHD para classifica√ß√£o do texto como fake ou n√£o fake.
Compara os ganhos de performance com a arquitetura apenas com ngram encoder.    

‚Ä¢ Notebook 3: BERT para Fake News  

Usa o modelo BERT pr√©-treinado para classifica√ß√£o bin√°ria de not√≠cias falsas.
Efetua fine-tuning no mesmo dataset dos experimentos HDC.
Serve como baseline do estado da arte para compara√ß√£o com HDC.
Sobre o Dataset  

‚Ä¢ Notebook 4: TFIDF e regress√£o logistica para Fake News  

Usa o modelo regress√£o logistica para classifica√ß√£o e encoder TFIDF bin√°ria de not√≠cias falsas.
Efetua fine-tuning no mesmo dataset dos experimentos HDC.
Serve como baseline do estado da arte para compara√ß√£o com HDC.  
Sobre o Dataset

O dataset cont√©m inicialmente 10000 linhas com not√≠cias reais e falsas.
Cada linha possui um texto (not√≠cia) e um r√≥tulo:
0 = Not√≠cia verdadeira
1 = Fake news
Objetivo

Comparar a simplicidade, desempenho e interpretabilidade entre m√©todos HDC e um Transformer (BERT) no contexto de fake news detection.
Auxiliar pesquisadores e entusiastas no entendimento pr√°tico dessas abordagens para NLP.
